{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "Bad CRC-32 for file 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mzip\u001b[39m\u001b[39m.\u001b[39mopen(filename) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 19\u001b[0m         dataframes\u001b[39m.\u001b[39mupdate({ filename : pd\u001b[39m.\u001b[39;49mread_csv(io\u001b[39m.\u001b[39;49mTextIOWrapper(f)) })\n\u001b[1;32m     21\u001b[0m         \u001b[39m# Lets take a look at the files\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFILE: \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m         nrows\n\u001b[1;32m   1706\u001b[0m     )\n\u001b[1;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:2017\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: Bad CRC-32 for file 'train.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "\n",
    "zipped_data_path = \"../data/clean_data/class-competition.zip\"\n",
    "\n",
    "dataframes = defaultdict(pd.DataFrame)\n",
    "with zipfile.ZipFile(zipped_data_path, \"r\") as zip:\n",
    "    for filename in zip.namelist():\n",
    "        if filename.endswith(\".csv\"):\n",
    "            with zip.open(filename) as f:\n",
    "                dataframes.update({ filename : pd.read_csv(io.TextIOWrapper(f)) })\n",
    "\n",
    "                # Lets take a look at the files\n",
    "                print(f\"FILE: {filename}\")\n",
    "                display(dataframes[filename].info())\n",
    "                display(dataframes[filename].head())\n",
    "\n",
    "train_data = dataframes[\"train.csv\"]\n",
    "test_data  = dataframes[\"test_public.csv\"]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# We could totally change this. Utilization of these just probably requires further preprocessing.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m ALL_FEATURES_NOT_SUITED_FOR_ESTIMATION \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mTRIP_ID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mORIGIN_CALL\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mORIGIN_STAND\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTAXI_ID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPOLYLINE\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m train_data_sample \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m0.02\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m420\u001b[39m) \u001b[39m# frac is used to control percentage of train data used\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X_sample \u001b[39m=\u001b[39m train_data_sample\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mTRAVEL_TIME\u001b[39m\u001b[39m\"\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m X_sample \u001b[39m=\u001b[39m X_sample\u001b[39m.\u001b[39mloc[:, \u001b[39m~\u001b[39mX_sample\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39misin(ALL_FEATURES_NOT_SUITED_FOR_ESTIMATION)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ALL_FEATURES = ['TRIP_ID', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID', 'MISSING_DATA',\n",
    " 'POLYLINE', 'CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C', 'YEAR_2013', 'YEAR_2014',\n",
    " 'MONTH_1', 'MONTH_2', 'MONTH_3', 'MONTH_4', 'MONTH_5', 'MONTH_6',\n",
    " 'MONTH_7', 'MONTH_8', 'MONTH_9', 'MONTH_10', 'MONTH_11', 'MONTH_12',\n",
    " 'DAY_OF_WEEK_0', 'DAY_OF_WEEK_1', 'DAY_OF_WEEK_2', 'DAY_OF_WEEK_3',\n",
    " 'DAY_OF_WEEK_4', 'DAY_OF_WEEK_5', 'DAY_OF_WEEK_6', 'HOUR_0', 'HOUR_1',\n",
    " 'HOUR_2', 'HOUR_3', 'HOUR_4', 'HOUR_5', 'HOUR_6', 'HOUR_7', 'HOUR_8',\n",
    " 'HOUR_9', 'HOUR_10', 'HOUR_11', 'HOUR_12', 'HOUR_13', 'HOUR_14',\n",
    " 'HOUR_15', 'HOUR_16', 'HOUR_17', 'HOUR_18', 'HOUR_19', 'HOUR_20',\n",
    " 'HOUR_21', 'HOUR_22', 'HOUR_23']\n",
    "\n",
    "# We could totally change this. Utilization of these just probably requires further preprocessing.\n",
    "ALL_FEATURES_NOT_SUITED_FOR_ESTIMATION = ['TRIP_ID', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID', 'POLYLINE']\n",
    "\n",
    "train_data_sample = train_data.sample(frac=0.01, random_state=420) # frac is used to control percentage of train data used\n",
    "X_sample = train_data_sample.drop(\"TRAVEL_TIME\", axis=1)\n",
    "X_sample = X_sample.loc[:, ~X_sample.columns.isin(ALL_FEATURES_NOT_SUITED_FOR_ESTIMATION)]\n",
    "y_sample = train_data_sample[\"TRAVEL_TIME\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(49, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    return self.layers(x)\n",
    "   \n",
    "mlp = MLP()\n",
    "if torch.cuda.is_available():\n",
    "    mlp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Set current loss value\u001b[39;00m\n\u001b[1;32m      6\u001b[0m current_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m xtrain \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(X_train\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m      8\u001b[0m xtrain \u001b[39m=\u001b[39m xtrain\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m xtrain \u001b[39m=\u001b[39m xtrain\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 10): # 10 epochs at maximum\n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    X_train = torch.tensor(X_train.values)\n",
    "    X_train = X_train.to(device)\n",
    "    X_train = X_train.float()\n",
    "    \n",
    "    y_train = torch.tensor(y_train.values)\n",
    "    y_train = y_train.to(device)\n",
    "    y_train = y_train.float()\n",
    "      \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Perform forward pass\n",
    "    outputs = mlp(X_train)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Perform optimization\n",
    "    optimizer.step()\n",
    "\n",
    "    current_loss += loss.item()\n",
    "    print(current_loss)\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
