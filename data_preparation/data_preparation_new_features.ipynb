{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE: metaData_taxistandsID_name_GPSlocation.csv\n",
      "FILE: sampleSubmission.csv\n",
      "FILE: test_public.csv\n",
      "FILE: train.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "import math\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "\n",
    "zipped_data_path = \"../data/raw_data/ucsd-cse-151b-class-competition.zip\"\n",
    "\n",
    "dataframes = defaultdict(pd.DataFrame)\n",
    "with zipfile.ZipFile(zipped_data_path, \"r\") as zipped:\n",
    "    for filename in zipped.namelist():\n",
    "        if filename.endswith(\".csv\"):\n",
    "            with zipped.open(filename) as f:\n",
    "                dataframes.update({ filename : pd.read_csv(io.TextIOWrapper(f)) })\n",
    "\n",
    "                # Lets take a look at the files\n",
    "                print(f\"FILE: {filename}\")\n",
    "                # If you want to see file info uncomment this:\n",
    "                # display(dataframes[filename].info())\n",
    "                # display(dataframes[filename].head())\n",
    "\n",
    "train_data = dataframes[\"train.csv\"]\n",
    "test_data  = dataframes[\"test_public.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_METADATA = dataframes[\"metaData_taxistandsID_name_GPSlocation.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1710670 entries, 0 to 1710669\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Dtype  \n",
      "---  ------        -----  \n",
      " 0   TRIP_ID       int64  \n",
      " 1   CALL_TYPE     object \n",
      " 2   ORIGIN_CALL   float64\n",
      " 3   ORIGIN_STAND  float64\n",
      " 4   TAXI_ID       int64  \n",
      " 5   TIMESTAMP     int64  \n",
      " 6   DAY_TYPE      object \n",
      " 7   MISSING_DATA  bool   \n",
      " 8   POLYLINE      object \n",
      "dtypes: bool(1), float64(2), int64(3), object(3)\n",
      "memory usage: 106.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate New Features\n",
    "Add travel time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def calculate_travel_time(polyline: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the travel time of a trip. Is defined as\n",
    "    (number of points - 1) * 15 seconds.\n",
    "    \n",
    "    :param polyline: The polyline of the trip.\n",
    "    :return: The travel time of the trip.\n",
    "    \"\"\"\n",
    "    return (len(json.loads(polyline)) - 1) * 15\n",
    "\n",
    "train_data[\"TRAVEL_TIME\"] = train_data[\"POLYLINE\"].apply(calculate_travel_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta_taxi_data(x):\n",
    "    if not math.isnan(x):\n",
    "        return str(TAXI_METADATA.at[x-1, \"Latitude\"]) +\",\" + str(TAXI_METADATA.at[x-1, \"Longitude\"])\n",
    "\n",
    "train_data[\"START_LOCATION\"] = train_data['ORIGIN_STAND'].apply(add_meta_taxi_data)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, col, max_val):\n",
    "    data[col + '_sin'] = np.sin(2 * np.pi * data[col]/max_val)\n",
    "    data[col + '_cos'] = np.cos(2 * np.pi * data[col]/max_val)\n",
    "    return data\n",
    "\n",
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "train_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = train_data[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "\n",
    "train_data = encode(train_data, 'MON', 12)\n",
    "train_data = encode(train_data, 'DAY', 31)\n",
    "train_data = encode(train_data, 'HR', 24)\n",
    "train_data = encode(train_data, 'WK', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"YR_2013\"] = train_data[\"YR\"] == 2013"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAD DATA\n",
    "Seemed like there was some weird recordings of data. Going to use to geocash to filter out data where taxi's are driving at crazy speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing data\n",
    "train_data = train_data[train_data[\"MISSING_DATA\"] != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty polyline\n",
    "train_data = train_data[train_data[\"POLYLINE\"] != \"[]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mean, std = train_data[\"TRAVEL_TIME\"].mean(), train_data[\"TRAVEL_TIME\"].std()\n",
    "median = train_data[\"TRAVEL_TIME\"].median()\n",
    "\n",
    "outlier_threshold = 3\n",
    "\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "train_data = train_data[train_data[\"TRAVEL_TIME\"] < mean + outlier_threshold * std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import distance\n",
    "\n",
    "speed_limit = 160  # Set the speed limit threshold in km/h\n",
    "time_diff = 1 / (15 / 3600)  # Each consecutive coordinate is 15 seconds apart. Convert units to hours. (storing as 1 / kmh for computational efficiency)\n",
    "\n",
    "def calculate_speed(latitude_1, longitude_1, latitude_2, longitude_2):\n",
    "    dist = distance((latitude_1, longitude_1), (latitude_2, longitude_2)).km\n",
    "    speed = dist * time_diff\n",
    "    return speed\n",
    "\n",
    "def calculate_speeds_with_missing_data(row):\n",
    "    coordinates = eval(row['POLYLINE'])\n",
    "    missing_data = False\n",
    "    for i in range(len(coordinates) - 1):\n",
    "        lat_1, lon_1 = coordinates[i][1], coordinates[i][0]\n",
    "        lat_2, lon_2 = coordinates[i + 1][1], coordinates[i + 1][0]\n",
    "        speed = calculate_speed(lat_1, lon_1, lat_2, lon_2)\n",
    "        if speed > speed_limit:\n",
    "            missing_data = True\n",
    "            break\n",
    "    row['BAD_DATA'] = missing_data\n",
    "    return row\n",
    "\n",
    "train_data = train_data.apply(calculate_speeds_with_missing_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BAD_DATA\n",
       "False    1544329\n",
       "True      142533\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"BAD_DATA\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[train_data[\"BAD_DATA\"] != True]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "remove unnecessary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['DAY_TYPE', 'MISSING_DATA', 'YR', 'WK', 'MON', 'DAY', 'HR', 'TIMESTAMP', 'BAD_DATA'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some START_LOCATIONS THAT ARE MESSED UP\n",
    "train_data.loc[train_data[\"ORIGIN_STAND\"] == 41, \"START_LOCATION\"] = \"41.163066654,-8.67598304213\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set\n",
    "gotta prep the test set now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"START_LOCATION\"] = test_data['ORIGIN_STAND'].apply(add_meta_taxi_data)\n",
    "\n",
    "test_data[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = test_data[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "\n",
    "test_data = encode(test_data, 'MON', 12)\n",
    "test_data = encode(test_data, 'DAY', 31)\n",
    "test_data = encode(test_data, 'HR', 24)\n",
    "test_data = encode(test_data, 'WK', 7)\n",
    "\n",
    "test_data[\"YR_2013\"] = test_data[\"YR\"] == 2013\n",
    "\n",
    "test_data.drop(['DAY_TYPE', 'MISSING_DATA', 'YR', 'WK', 'MON', 'DAY', 'HR', 'TIMESTAMP'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to split and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346466 731284 466579\n"
     ]
    }
   ],
   "source": [
    "train_data_call_type_A, train_data_call_type_B, train_data_call_type_C = train_data[train_data[\"CALL_TYPE\"] == \"A\"], train_data[train_data[\"CALL_TYPE\"] == \"B\"], train_data[train_data[\"CALL_TYPE\"] == \"C\"]\n",
    "\n",
    "# For some reason call_type_B has rows with origin_stand == NaN\n",
    "train_data_call_type_B = train_data_call_type_B[~train_data_call_type_B[\"ORIGIN_STAND\"].isna()]\n",
    "\n",
    "\n",
    "# lens of diff call type datasets\n",
    "print(len(train_data_call_type_A), len(train_data_call_type_B), len(train_data_call_type_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_train_zip = '../data/clean_data/class-competition-cleaned.zip'\n",
    "\n",
    "with zipfile.ZipFile(modified_train_zip, 'w') as zip:\n",
    "    zip.writestr(\"train_call_type_A.csv\", train_data_call_type_A.to_csv(index=False))\n",
    "    zip.writestr(\"train_call_type_B.csv\", train_data_call_type_B.to_csv(index=False))\n",
    "    zip.writestr(\"train_call_type_C.csv\", train_data_call_type_C.to_csv(index=False))\n",
    "    zip.writestr(\"test_public.csv\", test_data.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
