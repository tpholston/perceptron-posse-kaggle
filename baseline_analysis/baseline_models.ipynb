{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data_preparation.ipynb created train.zip which has train.csv inside\n",
    "zipped_data_path = \"../data/clean_data/class-competition.zip\"\n",
    "train_csv = \"train.csv\"\n",
    "test_csv = \"test_public.csv\"\n",
    "\n",
    "with zipfile.ZipFile(zipped_data_path, \"r\") as zip:\n",
    "    train_data = pd.read_csv(zip.open(train_csv))\n",
    "    test_data = pd.read_csv(zip.open(test_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ALL_FEATURES = ['TRIP_ID', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID', 'MISSING_DATA',\n",
    " 'POLYLINE', 'CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C', 'YEAR_2013', 'YEAR_2014',\n",
    " 'MONTH_1', 'MONTH_2', 'MONTH_3', 'MONTH_4', 'MONTH_5', 'MONTH_6',\n",
    " 'MONTH_7', 'MONTH_8', 'MONTH_9', 'MONTH_10', 'MONTH_11', 'MONTH_12',\n",
    " 'DAY_OF_WEEK_0', 'DAY_OF_WEEK_1', 'DAY_OF_WEEK_2', 'DAY_OF_WEEK_3',\n",
    " 'DAY_OF_WEEK_4', 'DAY_OF_WEEK_5', 'DAY_OF_WEEK_6', 'HOUR_0', 'HOUR_1',\n",
    " 'HOUR_2', 'HOUR_3', 'HOUR_4', 'HOUR_5', 'HOUR_6', 'HOUR_7', 'HOUR_8',\n",
    " 'HOUR_9', 'HOUR_10', 'HOUR_11', 'HOUR_12', 'HOUR_13', 'HOUR_14',\n",
    " 'HOUR_15', 'HOUR_16', 'HOUR_17', 'HOUR_18', 'HOUR_19', 'HOUR_20',\n",
    " 'HOUR_21', 'HOUR_22', 'HOUR_23']\n",
    "\n",
    "# We could totally change this. Utilization of these just probably requires further preprocessing.\n",
    "ALL_FEATURES_NOT_SUITED_FOR_ESTIMATION = ['TRIP_ID', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID', 'POLYLINE']\n",
    "\n",
    "X = train_data.drop(\"TRAVEL_TIME\", axis=1)\n",
    "X = X.loc[:, ~X.columns.isin(ALL_FEATURES_NOT_SUITED_FOR_ESTIMATION)]\n",
    "y = train_data[\"TRAVEL_TIME\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=420)\n",
    "\n",
    "\n",
    "# The test_data needs to have the proper features w/o TRIP_ID for estimation\n",
    "# test_features -> features for estimation\n",
    "# test_data -> whole dataframe including TRIP_ID for predction csv\n",
    "test_features = test_data.loc[:, ~test_data.columns.isin(ALL_FEATURES_NOT_SUITED_FOR_ESTIMATION)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "# This is gonna run for a really long time unless we operate on a sample \n",
    "def train_test_split_sample(frac, random_state=420):\n",
    "    # TODO: We should use stratified sampling here. Would help get more representative samples.\n",
    "    train_data_sample = train_data.sample(frac=frac, random_state=random_state) # frac is used to control percentage of train data used\n",
    "    X_sample = train_data_sample.drop(\"TRAVEL_TIME\", axis=1)\n",
    "    X_sample = X_sample.loc[:, ~X_sample.columns.isin(ALL_FEATURES_NOT_SUITED_FOR_ESTIMATION)]\n",
    "    y_sample = train_data_sample[\"TRAVEL_TIME\"]\n",
    "\n",
    "    X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(X_sample, y_sample, test_size=0.2, random_state=420)\n",
    "    return X_train_sample, X_test_sample, y_train_sample, y_test_sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: predicting average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692.1182957123148"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish a baseline for model comparison. What if we always predict the average travel time?\n",
    "avg_travel_time = y_train.mean()\n",
    "y_pred = [ avg_travel_time for i in range(len(X_test)) ]\n",
    "\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing Baseline:\n",
    "* RMSE = 678.8930754443071\n",
    "* the most naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction_to_csv(y_pred, outfile_name):\n",
    "    \n",
    "\toutput_df = pd.DataFrame(test_data[\"TRIP_ID\"])\n",
    "\toutput_df[\"TRAVEL_TIME\"] = y_pred\n",
    "\toutput_df.head()\n",
    "\n",
    "\toutput_df.to_csv(f'../guesses/{outfile_name}', index=False)\n",
    "\n",
    "test_prediction_to_csv(avg_travel_time, \"predicting_average_travel_time.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684.066273563164"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running a linear regression\n",
    "from sklearn import linear_model\n",
    "\n",
    "lreg = linear_model.LinearRegression()\n",
    "lreg.fit(X_train, y_train)\n",
    "y_pred = lreg.predict(X_test)\n",
    "\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405011e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405011e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405012e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405012e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405011e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405011e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405011e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405011e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405012e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405012e+11 4.79405011e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405011e+11 4.79405012e+11 4.79405011e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405011e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405012e+11 4.79405012e+11\n",
      " 4.79405012e+11 4.79405012e+11 4.79405011e+11 4.79405012e+11\n",
      " 4.79405011e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405012e+11 4.79405011e+11 4.79405011e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405012e+11 4.79405011e+11\n",
      " 4.79405011e+11 4.79405012e+11 4.79405011e+11 4.79405011e+11]\n"
     ]
    }
   ],
   "source": [
    "y_pred = lreg.predict(test_features)\n",
    "test_prediction_to_csv(y_pred, \"predicting_linear_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 763.585956808071\n",
      "{'alpha': 0.5179474679231213}\n"
     ]
    }
   ],
   "source": [
    "# Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "rmse = make_scorer(lambda y_true, y_pred: root_mean_squared_error(y_true, y_pred), greater_is_better=False)\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split_sample(frac=0.01)\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': np.logspace(-5, 2, 50)\n",
    "}\n",
    "\n",
    "search = GridSearchCV(lasso, param_grid, n_jobs = -1, scoring=rmse)\n",
    "search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f'Best CV score: {-1 * search.best_score_}')\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684.2282776160345"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Lasso\n",
    "lasso = Lasso(alpha = 0.5179474679231213)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso.predict(test_features)\n",
    "test_prediction_to_csv(y_pred, \"predicting_lasso_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 763.4839170992379\n",
      "{'alpha': 100.0}\n"
     ]
    }
   ],
   "source": [
    "# Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lasso = Ridge()\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': np.logspace(-5, 2, 50)\n",
    "}\n",
    "\n",
    "search = GridSearchCV(lasso, param_grid, n_jobs = -1, scoring=rmse)\n",
    "search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f'Best CV score: {-1 * search.best_score_}')\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684.0679267582618"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Ridge\n",
    "ridge = Ridge(alpha = 100)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ridge.predict(test_features)\n",
    "test_prediction_to_csv(y_pred, \"predicting_ridge_regression.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing Linear Regression:\n",
    "* Linear Regression: 671.0856201370245\n",
    "* Lasso Regression: 671.1443087427473\n",
    "* Ridge Regression: 671.0895985241863"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 674.4944260890286\n",
      "{'C': 68.12920690579611, 'kernel': 'rbf'}\n",
      "Best CV score: 674.2583457807992\n",
      "{'C': 68.12920690579611, 'degree': 2}\n"
     ]
    }
   ],
   "source": [
    "# !!!! SKIP THIS IF YOU DON'T WANT TO WAIT A LONG TIME !!!!\n",
    "\n",
    "# Running a SVM. Using 0.1% of original data runs in ~2mins on my laptop, 0.5% runs in ~23mins.\n",
    "# This can be skipped unless you really want to rerun it. 0.5% yielded:\n",
    "# Best CV score: 600.6364736351106\n",
    "# {'C': 100.0, 'kernel': 'linear'}\n",
    "# so just using this for now.\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split_sample(frac=0.01, random_state=500)\n",
    "\n",
    "svm = SVR()\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(0, 2, 25),\n",
    "    'kernel': ['rbf', 'sigmoid', 'linear']\n",
    "}\n",
    "\n",
    "search = GridSearchCV(svm, param_grid, n_jobs = -1, scoring=rmse)\n",
    "search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f'Best CV score: {-1 * search.best_score_}')\n",
    "print(search.best_params_)\n",
    "\n",
    "svm_poly = SVR(kernel = 'poly')\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.logspace(0, 2, 25),\n",
    "    'degree': np.arange(1, 4)\n",
    "}\n",
    "\n",
    "search = GridSearchCV(svm_poly, param_grid, n_jobs = -1, scoring=rmse)\n",
    "search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f'Best CV score: {-1 * search.best_score_}')\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683.8392104356353"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the SVM on 10% of data (8% train/2% validation)\n",
    "from sklearn.svm import SVR\n",
    "svm = SVR(kernel = 'poly', C=68.12920690579611, degree=2)\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split_sample(frac=0.1, random_state=500)\n",
    "svm.fit(X_train_sample, y_train_sample)\n",
    "y_pred = svm.predict(X_test_sample)\n",
    "\n",
    "root_mean_squared_error(y_test_sample, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(test_features)\n",
    "test_prediction_to_csv(y_pred, \"predicting_svm_poly_kernel_ten_percent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804.1847346133947"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That was good but does it generalize to another larger sample? - Yes, it does!\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split_sample(frac=0.015)\n",
    "\n",
    "svm = SVR(kernel = 'linear', C=100)\n",
    "\n",
    "svm.fit(X_train_sample, y_train_sample)\n",
    "y_pred = svm.predict(X_test_sample)\n",
    "\n",
    "root_mean_squared_error(y_test_sample, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing SVM:\n",
    "* SVM did well on 0.5% of data w/ RSME = 738.95\n",
    "* seemed to perform worse with more data (1.5%) w/ RSME = 804.18\n",
    "* could try some other sample sizes on not my shitty laptop\n",
    "* UPDATE: After trying on my PC, was able to get higher train set which yielded better performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 696.7578957269487\n",
      "{'n_estimators': 19}\n"
     ]
    }
   ],
   "source": [
    "# Bagging w/ Decision Trees\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split_sample(frac=0.1)\n",
    "\n",
    "bag = BaggingRegressor(estimator = DecisionTreeClassifier(), random_state = 420)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(1,20))\n",
    "}\n",
    "\n",
    "search = GridSearchCV(bag, param_grid, n_jobs = -1, scoring = rmse)\n",
    "search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f'Best CV score: {-1 * search.best_score_}')\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754.9926609619301"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the Bagging Regressor\n",
    "bag = BaggingRegressor(estimator = DecisionTreeClassifier(), random_state = 420, n_estimators=19)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "y_pred = bag.predict(X_test)\n",
    "\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bag.predict(test_features)\n",
    "test_prediction_to_csv(y_pred, \"predicting_bagging_regressor_19_estimators.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing Bagging:\n",
    "* RSME = 755.15 on all training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 766.9098843101626\n",
      "{'n_estimators': 1200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 60, 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split_sample(frac=0.01)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=420)\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(rf, param_grid, n_jobs = -1, scoring = rmse)\n",
    "search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "print(f'Best CV score: {-1 * search.best_score_}')\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738.5281634010671"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the Random Forest\n",
    "rf = RandomForestRegressor(random_state=420, max_features='sqrt', n_estimators = 1200, min_samples_leaf=4, min_samples_split=10, max_depth=60, bootstrap=True)\n",
    "\n",
    "rf.fit(X_train_sample, y_train_sample)\n",
    "y_pred = rf.predict(X_test_sample)\n",
    "\n",
    "root_mean_squared_error(y_test_sample, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "681.5772344495936"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# THIS ONE TAKES A LONG TIME TOO !!!!\n",
    "rf = RandomForestRegressor(random_state=420, max_features='sqrt', n_estimators = 1200, min_samples_leaf=4, min_samples_split=10, max_depth=60, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All train data\n",
    "rf = RandomForestRegressor(random_state=420, max_features='sqrt', n_estimators = 1200, min_samples_leaf=4, min_samples_split=10, max_depth=60, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(test_features)\n",
    "test_prediction_to_csv(y_pred, \"predicting_random_forest.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewing Random Forest:\n",
    "* RSME = 668.6428710752018 on all training data\n",
    "* solid baseline to try and beat with deep learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
